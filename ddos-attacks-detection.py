# -*- coding: utf-8 -*-
"""Major project code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mX_iaFWDI5aPX-QORUfBRH-Im9wSYz7f
"""

import matplotlib.pyplot as plt
import seaborn as sb
import numpy as пр
import pandas as pd
import zipfile
import seaborn as sns

from google. colab import drive
drive.mount('/content/drive' ,force_remount=True)

zf = zipfile.ZipFile('/content/drive/My Drive/Colab Notebooks/CICDDOS2019.zip')
zf.extractall("/content/drive/My Drive/Colab Notebooks")

cd /content/drive/My Drive/Colab Notebooks

import matplotlib.pyplot as plt
import seaborn as sb
import numpy as np
import pandas as pd

d1 = pd.read_parquet('LDAP-training.parquet')
d1.shape

d1 = pd.read_parquet('DNS-testing.parquet')
d1 = d1.sample(n=6000, random_state=24)
d2 = pd.read_parquet('LDAP-training.parquet')
d2 = d2.sample(n=6000, random_state=24)
d3 = pd.read_parquet('MSSQL-training.parquet')
d3 = d3.sample(n=6000, random_state=24)
d4 = pd.read_parquet('NetBIOS-testing.parquet')
d4 = d4.sample(n=2000, random_state=24)
d5 = pd.read_parquet('NTP-testing.parquet')
d5 = d5.sample(n=6000, random_state=24)
d6 = pd.read_parquet('SNMP-testing.parquet')
d6 = d6.sample(n=4000, random_state=24)
d7 = pd.read_parquet('Portmap-training.parquet')
d7 = d7.sample(n=5000, random_state=24)
d8 = pd.read_parquet('UDP-training.parquet')
d8 = d8.sample(n=6000, random_state=24)
d9 = pd.read_parquet('Syn-training.parquet')
d9 = d9.sample(n=6000, random_state=24)
d10 = pd.read_parquet('TFTP-testing.parquet')
d10 = d10.sample(n=6000, random_state=24)
d11 = pd.read_parquet('UDPLag-training.parquet')
d11 = d11.sample(n=6000, random_state=24)

# d1 = pd.read_parquet('01-12/DrDoS_DNS.csv' ,nrows=200000)
# d1 = d1.sample(n=40000, random_state=24)
# d2 = pd.read_csv('01-12/DrDoS_LDAP.csv' ,nrows=200000)
# d2 = d2.sample(n=40000, random_state=24)
# d3 = pd.read_csv('01-12/DrDoS_MSSQL.csv' ,nrows=200000)
# d3 = d3.sample(n=40000, random_state=24)
# d4 = pd.read_csv('01-12/DrDoS_NetBIOS.csv' ,nrows=200000)
# d4 = d4.sample(n=40000, random_state=24)
# d5 = pd.read_csv('01-12/DrDoS_NTP.csv' ,nrows=200000)
# d5 = d5.sample(n=40000, random_state=24)
# d6 = pd.read_csv('01-12/DrDoS_SNMP.csv' ,nrows=200000)
# d6 = d6.sample(n=40000, random_state=24)
# d7 = pd.read_csv('01-12/DrDoS_SSDP.csv' ,nrows=200000)
# d7 = d7.sample(n=40000, random_state=24)
# d8 = pd.read_csv('01-12/DrDoS_UDP.csv' ,nrows=200000)
# d8 = d8.sample(n=40000, random_state=24)
# d9 = pd.read_csv('01-12/Syn.csv' ,nrows=200000)
# d9 = d9.sample(n=40000, random_state=24)
# d10 = pd.read_csv('01-12/TFTP.csv' ,nrows=200000)
# d10 = d10.sample(n=40000, random_state=24)
# d11 = pd.read_csv('01-12/UDPLag.csv' ,nrows=200000)
# d11 = d11.sample(n=40000, random_state=24)

df = pd. concat ([d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11])

data = df

def Pre_process_data(df,col):
      '''
      Input: Data-frame and Column name.
     Operation: Fills the nan values with the minimum value in their respective column.
      Output: Returns the pre-processed data-frame.
      '''
      #df[ ‘primary use’] = df[‘primary_use’].astype("category").cat.codes
      print("Name of column with NaN: " +str(col))
      print(df[col].value_counts(dropna=False, normalize=True) .head())
      df[col].replace(np.inf, -1, inplace=True)
      return df

def reduce_mem_usage(df):
  '''
  Input - data- frame.
  Operation - Reduce memory usage of the data-frame.
  '''
  start_mem_usg = df.memory_usage().sum() / 1024**2
  print("Memory usage of properties dataframe is :",start_mem_usg," MB")
  #NAlist = [] # Keeps track of columns that have missing values filled in.
  for col in df.columns:
    if df[col].dtype != object: # Exclude strings
      # Print current column type
      print ("*****************************")
      print("Column: ",col)
      print("dtype before: ",df[col].dtype)
      # make variables for Int, max and min
      IsInt = False

      mx = df[col].max()
      mn = df[col].min()
      #print("min for this col: ",mn)
      #print("max for this col: ",mx)
    
      # Integer does not support NA, therefore, NA needs to be filled
      if not np.isfinite(df[col]).all():
        #NAList. append(col)
        df = Pre_process_data(df,col)

      # test if column can be converted to an integer
      asint = df[col].fillna(0).astype(np.int64)
      result = (df[col] - asint)
      result = result.sum()
      if result > -0.01 and result < 0.01:
        IsInt = True
      # Make Integer/unsigned Integer datatypes
      if IsInt:
        if mn >= 0:
          if mx < 255:
            df[col] = df[col].astype(np.uint8)
          elif mx < 65535:
            df[col] = df[col].astype(np.uint16)
          elif mx < 4294967295:
            df[col] = df[col].astype(np.uint32)
          else:
            df[col] = df[col].astype(np.uint64)
        else:
          if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:
            df[col] = df[col].astype(np.int8)
          elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:
            df[col] = df[col].astype(np.int16)
          elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:
            df[col] = df[col].astype(np.int32)
          elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:
            df[col] = df[col].astype(np.int64)
      # Make float datatypes 32 bit
      else:
        df[col] = df[col].astype(np.float32)

      # Print new column type
      print("dtype after: ",df[col].dtype)
      print("*******************")

  # Print final result
  print("___MEMORY USAGE AFTER COMPLETION: __")
  mem_usg = df.memory_usage().sum() / 1024**2
  print("Memory usage is: ",mem_usg,"” MB")
  print("This is ",100*mem_usg/start_mem_usg,"% of the initial size")
  return df

df = reduce_mem_usage(df)

"""Visualisations"""

# Count of each attack using value_counts() function
data_ = df
df['Label'].value_counts()

#  Showing distribution of different attacks using pie chart
labels = 'Benign', 'UDP', 'Syn', 'DrDoS_SSDP','DrDoS_NTP' ,'TFTP','MSSQL','DrDoS_DNS','DrDoS_SNMP','LDAP','Portmap','DrDoS_NetBIOS','NetBIOS','UDPLag'
sizes = [len(data_[data_['Label']=='Benign']),
        len(data_[data_['Label']=='UDP']),
        len(data_[data_['Label']=='Syn']),
        len(data_[data_['Label']=='DrDoS_SSDP']),
        len(data_[data_['Label']=='DrDoS_NTP']),
        len(data_[data_['Label']=='TFTP']),
        len(data_[data_['Label']=='MSSQL']),
        len(data_[data_['Label']=='DrDoS_DNS']),
        len(data_[data_['Label']=='DrDoS_SNMP']),
        len(data_[data_['Label']=='LDAP']),
        len(data_[data_['Label']=='Portmap']),
        len(data_[data_['Label']=='DrDoS_NetBIOS']),
        len(data_[data_['Label']=='NetBIOS']),
        len(data_[data_['Label']=='UDPLag'])]
colors = ['#da58d1','#fa7011','#58b065','#b66302','#fe0085','#301650','#de9d01','#b6f6f0','#4ff04d','#09b2af','#d73a29','#e84693','#c2b68e','#99ff99']
#colors = ['gold', 'yellowgreen','lightcoral','lightskyblue','yellow','purple','grey',' red','orange','black','violet','magenta','white']
explode = (0.3, 0.1, 0.1, 0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.3,0.2,0.1,0.1) # explode Ist slice

     

# Plot
plt.rcParams.update({'font.size':22})
plt.figure(figsize=(10,10))
plt.pie(sizes, explode = explode , labels=labels, colors=colors, autopct='%1.2f%%', shadow=True, startangle=140)

plt.axis('equal')
plt.show()

'''
labels = 'Benign', 'UDP', 'Syn', 'DrDoS_SSDP','DrDoS_NTP' ,'TFTP','MSSQL','DrDoS_DNS',
'DrDoS_SNMP','LDAP','Portmap','DrDoS_NetBIOS','NetBIOS','UDPLag'
len(labels)
'''





plt.figure(figsize=(40,20))

g1 = sns.countplot(x='Label', hue='Label', data=data_)

gt = g1.twinx()

gt = sns.pointplot(y='Flow Packets/s', x='Label', data=data_, color='black')
gt.set_ylabel('Flow Packets/s', fontsize=16)

plt.figure(figsize=(40,16))

g1 = sns.countplot(x='Label', hue='Label', data=data_)

gt = g1.twinx()

gt = sns.pointplot(y='Flow Bytes/s', x='Label', data=data_, color='black')
gt.set_ylabel('Flow Bytes/s', fontsize=16)



plt.figure(figsize=(40,16))
gl = sns.scatterplot(y='Total Fwd Packets', x='Fwd Packets Length Total',
                    sizes=(200, 400), size='Flow Duration' ,data=data_)

gt = gl.twinx()
t = sns.pointplot(y='Fwd Packets/s', x='Label', data=data_, color='black')
gt.set_ylabel('Fwd Packets/s', fontsize=16)

plt.figure(figsize=(40,16))
g1 = sns.scatterplot(y='Total Backward Packets', x='Bwd Packets Length Total',
                    sizes=(200, 400), size='Flow Duration' ,data=data_)

gt = g1.twinx()
t = sns.pointplot(y='Bwd Packets/s', x='Label', data=data_, color='black')
gt.set_ylabel('Bwd Packets/s', fontsize=16)

plt. figure(figsize=(20,16))
g1 = sns.countplot(x='Label', hue='Label', data=data_)

gt = g1.twinx()
gt = sns.countplot(x='Protocol', hue='Label',alpha=0.7, data=data_)

# gt.set_ylabel(" P", fontsize=16)
# Protocols: 0:hop-to-hop, 6:TCP, 17:UDP
#  From the graph, it is clear that most of the attacks happen in UDP (protocol=17) because it is a connectionless protocol and thus it is easier to carry out DDoS Attacks in UDP

df.columns

# plt.figure(figsize=(20,16))

# g1 = sns.countplot(x='Label', data=data_,alpha=0.5)

# gt = g1.twinx()

# gt = sns.countplot(x='  Inbound', hue='Label' ,alpha=0.7, data=data_)
# gt.set_ylabel('  Inbound', fontsize=16)

from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
# X_std = StandardScaler().fit_transform(df)

y = df['Label']
df = df.drop(['Fwd Packets/s','Flow Bytes/s','Label'], axis=1)

X = StandardScaler().fit_transform(df)
X_norm = preprocessing.normalize(X)

# df[' Inbounds'].value_counts()

"""Correlation"""

# Plotting Correlation Matrix
f = plt.figure(figsize=(20, 15))
plt.matshow(df.corr(), fignum=f.number)
plt.xticks(range(df.shape[1]), df.columns, fontsize=10, rotation=90)
plt.yticks(range(df.shape[1]), df.columns, fontsize=10)

cb = plt.colorbar()

cb.ax.tick_params(labelsize=14)

# Printing pairs of features which have pearson's correlation above 0.9
from scipy import stats
total = 0
count = 0
for i in df.columns:
  for j in df.columns:
    if i!=j:
      corr, _ = stats.pearsonr(data[i], data[j])
      total = total + 1

      if corr>0.9:
        count = count +1
        print('Pearsons correlation between '+i+' and '+j+': %.3f' % corr)
print(count, total)
print(count/total)
# As can be seen, we have 98 such pairs which are highly correlated, thus causing redundancy.

from sklearn.preprocessing import StandardScaler
X_std = StandardScaler(). fit_transform(df)

mean_vec = np.mean(X, axis=0)

cov_mat = ((X - mean_vec) .T.dot(X - mean_vec)) / (X.shape[0]-1)

print( 'Covariance matrix \n%s' %cov_mat)











"""Applying transformations"""

# UDP data
((pd.DataFrame(np.log(data.where(data['Label']==0).astype('float')))).replace(-np.Inf,0)).hist(figsize=(30, 30),xlabelsize=8, ylabelsize=8);

#Syn

((pd.DataFrame(np.log(data.where(data['Label']==2).astype('float')))).replace(-np.Inf,0)).hist(figsize=(30, 30),xlabelsize=8, ylabelsize=8);



"""Hypothesis Testing

Hypothesis Testing is a statistical method used to determine a hypothesis, i.e, an assumption about the distribution of data to be true or false.

HO: There exists no relation between the two features

H1: There exists a relation between the featurgs under consideration

Chi Squared Test
"""

#Relation between SYN Flag Count and Protocol
from scipy.stats import chi2_contingency
stat, p, dof, expected = chi2_contingency(pd.crosstab(data['SYN Flag Count' ],data['Protocol' ]))
alpha = 0.05
print("p value is " + str(p))
print(' SYN Flag Count',' Protocol')
if p > alpha:
  print( 'Independent (H0 holds true)')
else:
  print( 'Dependent (reject H0)')

#Relation between PSH Flag Count and Protocol
from scipy.stats import chi2_contingency
stat, p, dof, expected = chi2_contingency(pd.crosstab(data['PSH Flag Count'],data['Protocol']))
alpha = 0.05
print("p value is " + str(p))
print(' PSH Flag Count and Protocol')
if p > alpha:
  print('Independent (H0 holds true)')
else:
  print('Dependent (reject H0)')

#Relation between RST Flag Count and Protocol
from scipy.stats import chi2_contingency
stat, p, dof, expected = chi2_contingency(pd.crosstab(data['RST Flag Count' ],data['Protocol' ]))
alpha = 0.05
print("p value is " + str(p))
print(' RST Flag Count and Protocol')
if p > alpha:
  print( 'Independent (H0 holds true)')
else:
  print('Dependent (reject H0)')

pd.crosstab(data['RST Flag Count'],data['Protocol' ])

"""T Test"""

#T test
# If score>0.05 there exists a relation between the columns 
from scipy.stats import ttest_ind
score = ttest_ind(data['Flow Duration'], data['Fwd IAT Total'], equal_var = False)[1]
print(score)

# #Plots of Flow Duration and Fwd IAT Total
# plt.hist(data['Flow Duration'].where(data['Label']==1), 30,ls='dashed', lw=3, fc=(0,0,1,0.5))
# plt.hist(data['Flow Duration'].where(data['Label']==1), 30,ls='dashed', lw=3, fc=(1,0,0,0.5))
# plt.title('Fwd IAT Total')
# plt.show()

"""Principal Component Analysis"""

from sklearn.preprocessing import StandardScaler
X_std= StandardScaler().fit_transform(df)
mean_vec = np.mean(X, axis=0)
cov_mat= (X- mean_vec).T.dot((X- mean_vec))/ (X.shape[0]-1)
print('Covariance matrix \n%s' %cov_mat)

eig_vals, eig_vecs = np.linalg.eig(cov_mat)
print('Eigenvectors \n%s' %eig_vecs)
print('\nEigenvalues \n%s' %eig_vals)

eig_pairs =[(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]

#Sort the (eigenvalue, eigenvector) tuples from high to low
eig_pairs.sort(key=lambda x:x[0], reverse =True)

print('Eigenvalues in descending order:')
for i,j in enumerate(eig_pairs):
  print(i,j[0])

tot= sum(eig_vals)
var_exp=[(i/tot)*100 for i in sorted(eig_vals, reverse=True)]

with plt.style.context('dark_background'):
  plt.figure(figsize=(30,30))
  plt.bar(range(75), var_exp, alpha =0.5, align='center', label='individual explained variance')
  plt.ylabel('Explained varianc ratio')
  plt.xlabel('Principal components')
  plt.legend(loc='best')
  plt.tight_layout()

matrix_w=np.hstack((eig_pairs[0][1].reshape(75,1), eig_pairs[1][1].reshape(75,1)))
print('Matrix W:\n', matrix_w)

Y= X_std.dot(matrix_w)
Y

from sklearn.decomposition import PCA
pca =PCA().fit(X_std)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,75)
plt.xlabel('Number of Components',  fontsize=18)
plt.ylabel('Cumulative explained variance', fontsize=18)

from sklearn.decomposition import PCA
pca =PCA(n_components=2)
principalComponents= pca.fit_transform(X_norm)
plt.figure(figsize=(16,16))
g1=sns.scatterplot(x=principalComponents[:,0], y=principalComponents[:,1], s=100, hue=data_['Label'], alpha=0.7)
plt.title('Visualising DDos attacks through PCA', fontsize=24);

"""# Supervised Learning Models"""

X=df_X

X.shape

from imblearn.over_sampling import SMOTE
from collections import Counter
from matplotlib import pyplot
from sklearn.preprocessing import LabelEncoder
from sklearn import utils
from sklearn.utils import _safe_indexing

y=LabelEncoder().fit_transform(y)

oversample=SMOTE()

X, y=oversample.fit_resample(X,y)
counter=Counter(y)
for k,v in counter.items():
  per =v/len(y)*100
  print('Class=%d, n=%d (%.3f%%)' %(k,v,per))

pyplot.bar(counter.keys(), counter.values())
pyplot.show()

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, accuracy_score
X_train, X_test, y_train, y_test =train_test_split(X,y, test_size=0.25, random_state=1)

from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.multiclass import OneVsRestClassifier

model= LogisticRegression(max_iter= 440000)

ovr=OneVsRestClassifier(model)

ovr.fit(X_train,y_train)

y_pred=ovr.predict(X_test)

print(classification_report(y_test, y_pred,labels=np.unique(y_pred)))

!pip install scikit-plot
import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(y_test,y_pred)

from sklearn.tree import DecisionTreeClassifier
from matplotlib import pyplot as plt
from sklearn import tree
classifier=DecisionTreeClassifier()
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))

print(accuracy_score(y_test,y_pred))

columns=df.columns
cn=['Syn','DrDoS_SNMP','DrDoS_LDAP','DrDoS_SSDP','DrDoS_NetBIOS', 'DrDoS_MSSQL', "TFTP",'DrDoS_UDP', 'DrDoS_DNS', 'UDP-lag','DrDoS_NTP','BENIGN','WebDDoS']
import sys
sys.setrecursionlimit(10**9)
import graphviz

dot_data =tree.export_graphviz(classifier, out_file=None, feature_names=columns, class_names=cn, filled=True)

graph= graphviz.Source(dot_data, format='png')
graph

# graph.render("/tmp/01-02/decision_tree_graphviz")

"""

```
# This is formatted as code
```

Random Forest"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 5000, criterion = "entropy", random_state = 0)
classifier.fit(X_train, y_train)
# Predicting the rest set results
Y_Pred = classifier.predict(X_test)
# Making the confusion Matrix
print(classification_report(y_test, Y_Pred))

print(confusion_matrix(y_test, y_pred))

import scikitplot as skplt
skplt.metrics.plot_confusion_matrix(y_test, y_pred, figsize=(16,16))

"""K Nearest Neighbours"""

import sklearn
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.20)

classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors = 3)
classifier.fit(X_train, y_train)
Accuracy=[]
Accuracy.append(classifier.score(X_test, y_test))
y_pred = classifier.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt

K=classifier.kneighbors(X_test,3, False)
plt.plot(3, Accuracy, color = "red", marker = "o")
plt.title("Accuracy vs k", fontsize = 14)
plt.xlabel("K", fontsize = 14)
plt.ylabel("Accuracy", fontsize = 14)
plt.grid(True)
plt.show()

from scipy.interpolate import make_interp_spline, BSpline

# 300 represents number of points to make between T.min and T.max
xnew = np.linspace(3, 100, 1000)

spl = make_interp_spline(K, Accuracy, k=1)  # type: BSpline
power_smooth = spl(xnew)

plt.title("Accuracy vs number of neighbours", fontsize = 14)
plt.xlabel("K or number of neighbours", fontsize = 14)
plt.ylabel("Accuracy", fontsize = 14)
plt.plot(xnew, power_smooth)
plt.show()

y_pred = classifier.predict(X_test)

df['Label']=y
class_names = pd.unique(df["Label"])

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import plot_confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

titles_options = [("Confusion matrix, without normalization", None)]

for title, normalize in titles_options:
  fig, ax = plt.subplots(figsize=(20,16))
  disp = plot_confusion_matrix(classifier, X_test, y_test, display_labels = class_names, cmap = plt.cm.Blues, normalize = normalize, ax = ax)
  disp.ax_.set_title(title)
  plt.xticks(rotation = 90)

  print(title)
  print(disp.confusion_matrix)

plt.show()

PDF=df
class_names = pd.unique(PDF["Label"])

#PDF = PDF.drop(["unnamed: 0", "Flow ID", " Source IP", " Source Port", "SimilarHTTP", "Destination IP", "Destination Port", "Timestamp", "Fwd Packets/s", "Flow Bytes/s"], axis = 1)
colList = [i for i in PDF.columns]

# we cannot normalize a Dataframe with a column with data type Object
# So, we give ordinal number to each

enc = sklearn.preprocessing.OrdinalEncoder(dtype = "int8").fit_transform(PDF)

N = pd.DataFrame(enc, columns = colList)
B = N["Label"]
A = N.drop(["Label"], axis = 1)

B = pd.DataFrame(B, columns = [" Label"])

X = StandardScaler().fit_transform(A)
X_norm = np.array(preprocessing.normalize(X))
X_norm = pd.DataFrame(X_norm, columns = colList[0:75])
X_norm["Label"] = B
y = N["Label"]

import math
cmap_light = ["#BA6BCB", "#7986CB", "#4FC3F7", "#4DB6AC", "#81C784", "#AED581", "#FFF176", "#FFD54F", "#FFB74D", "#E0E0E0", "#90A4AE", "#EE98F8"]
cmap_bold = ["BE24AA", "#3949AB", "#039BE5", "#00897B", "#43A047", "#7CB342", "#FDD835", "#FFB300", "#FB8C00", "#757575", "#546E7A", "#883997"]

for a in colList:
  for b in colList:
    if a != b:
      # we create an instance of neighbours classifier and fit the data
      clf = sklearn.neighbors.KNeighborsClassifier(3, weights = "uniform")

      X = X_norm[[a, b]]
      X = np.array(X)
      clf.fit(X, np.array(y))

      xx = np.array([[i/300, j/300] for i in range(math.floor(X[:,0].min()*300), math.ceil(X[:,0].max()*300), 2) for j in range(math.floor(X[:, i].min()*300), math.ceil(X[:,i].max()*300), 2)])
      print("Started between {} and {}".format(a,b))
      Z = clf.predict(xx)
      print("Ended")

      Q = pd.DataFrame(X)
      P = pd.DataFrame(xx)

      fig = plt.figure(figsize = (8,6))
      for i in range(int(Z.size)):
        plt.scatter(x = P.iloc[i][0], y = P.iloc[i][1], c = cmap_light[int(Z.iloc[i][0])])
      for i in range(int(y.size)):
        plt.scatter(x = Q.iloc[i][0], y = Q.iloc[i][1], c = cmap_light[int(y.iloc[i][0])])
        
      # Plot also the training points
      # sns.scatterplot(x = X[:,0], y = X[:,1], hue = class_names, palette = cmap_bold, alpha = 1.0, edgecolor = "black")
      plt.xlim(math.floor(X[:,0].min()*300), math.ceil(X[:,0].max()*300))
      plt.ylim(math.floor(X[:,1].min()*300), math.ceil(X[:,1].max()*300))
      plt.title("11-class classification (k=%i, weights = '%s')" % (3, weights))
      plt.xlabel(a)
      plt.ylabel(b)

plt.show()

y = pd.DataFrame(y)
y

#P.size

import matplotlib.pyplot as plt
from numpy.random import random

cmap_light = ["#BA6BCB", "#7986CB", "#4FC3F7", "#4DB6AC", "#81C784", "#AED581", "#FFF176", "#FFD54F", "#FFB74D", "#E0E0E0", "#90A4AE", "#EE98F8"]
cmap_bold = ["#BE24AA", "#3949AB", "#039BE5", "#00897B", "#43A047", "#7CB342", "#FDD835", "#FFB300", "#FB8C00", "#757575", "#546E7A", "#883997"]

fig = plt.figure(figsize=(12,10))

A = plt.scatter(0, 1, marker = "+", s = 200, color = cmap_light[0])
B = plt.scatter(0, 2, marker = "+", s = 200, color = cmap_light[1])
C = plt.scatter(0, 3, marker = "+", s = 200, color = cmap_light[2])
D = plt.scatter(0, 4, marker = "+", s = 200, color = cmap_light[3])
E = plt.scatter(0, 5, marker = "+", s = 200, color = cmap_light[4])
F = plt.scatter(0, 6, marker = "+", s = 200, color = cmap_light[5])
G = plt.scatter(0, 7, marker = "+", s = 200, color = cmap_light[6])
H = plt.scatter(0, 8, marker = "+", s = 200, color = cmap_light[7])
I = plt.scatter(0, 9, marker = "+", s = 200, color = cmap_light[8])
J = plt.scatter(0, 10, marker = "+", s = 200, color = cmap_light[9])
K = plt.scatter(0, 11, marker = "+", s = 200, color = cmap_light[10])
L = plt.scatter(0, 12, marker = "+", s = 200, color = cmap_light[11])


a = plt.scatter(1, 1, marker = "+", s = 200, color = cmap_bold[0])
b = plt.scatter(1, 2, marker = "+", s = 200, color = cmap_bold[1])
c = plt.scatter(1, 3, marker = "+", s = 200, color = cmap_bold[2])
d = plt.scatter(1, 4, marker = "+", s = 200, color = cmap_bold[3])
e = plt.scatter(1, 5, marker = "+", s = 200, color = cmap_bold[4])
f = plt.scatter(1, 6, marker = "+", s = 200, color = cmap_bold[5])
g = plt.scatter(1, 7, marker = "+", s = 200, color = cmap_bold[6])
h = plt.scatter(1, 8, marker = "+", s = 200, color = cmap_bold[7])
i = plt.scatter(1, 9, marker = "+", s = 200, color = cmap_bold[8])
j = plt.scatter(1, 10, marker = "+", s = 200, color = cmap_bold[9])
k = plt.scatter(1, 11, marker = "+", s = 200, color = cmap_bold[10])
l = plt.scatter(1, 12, marker = "+", s = 200, color = cmap_bold[11])

plt.legend((A, a, B, b, C, c, D, d, E, e, F, f, G, g, H, h, I, i, J, j, K, k, L, l),
           (
              'UDP-lag',
              'Possibility',

              'DrDoS_LDAP',
              'Possibility',

              'DrDoS_SSDP',
              'Possibility',

              'DrDoS_UDP',
              'Possibility',

              'DrDoS_NetBIOS',
              'Possibility',

              'DrDoS_SNMP',
              'Possibility',

              'DrDoS_MSSQL',
              'Possibility',

              'TFTP',
              'Possibility',

              'Syn',
              'Possibility',

              'DrDos_DNS',
              'Possibility',

              'DrDos_NTP',
              'Possibility',

              'BENIGN',
              'Possibility',
        ),
        scatterpoints = 1,
        loc = "center",
        ncol = 2,
        fontsize = 10)

plt.show()

"""# Feature Scaling

## Standard Scaler
"""

from sklearn.preprocessing import StandardScaler
from sklearn import preprocessing
from sklearn.preprocessing import OrdinalEncoder

#df = df.drop(["SimilarHTTP"], axis = 1)

"""## Feature Scaling"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Sklearn lib
from sklearn import preprocessing
df.head()

# to handle values with varying magnitude
x = df.iloc[:, 1:3].values
print("Original data values:\n", x)

from sklearn import preprocessing
# Min Max Scaler
min_max_scaler = preprocessing.MinMaxScaler(feature_range = (0,1))
x_after_min_max_scaler = min_max_scaler.fit_transform(x)
print("After min and max scaling:\n", x_after_min_max_scaler)

# Standardisation
Standardisation = preprocessing.StandardScaler()
x_after_Standardisation = Standardisation.fit_transform(x)
print("After Standardisation:\n", x_after_Standardisation)

# Python code for Feature Scaling using Robust Scaling

# 1. Importing Libs
import pandas as pd
import numpy as np
from sklearn import preprocessing
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
matplotlib.style.use("ggplot")

# 2. Making the data distributions
x = pd.DataFrame({
    # Distribution with lower outliers
    'x1': np.concatenate([np.random.normal(20, 1, 2000), np.random.normal(1, 1, 20)]),
    # Distribution with higheroutliers
    'x2': np.concatenate([np.random.normal(30, 1, 2000), np.random.normal(50, 1, 20)]),
})

# 3. Scaling the data
scaler = preprocessing.RobustScaler()
robust_scaled_df = scaler.fit_transform(x)
robust_scaled_df = pd.DataFrame(robust_scaled_df, columns = ["x1", "x2"])

fig, (ax1, ax2, ax3) = plt.subplots(ncols = 3, figsize = (9,5))
ax1.set_title("Before Scaling")
sns.kdeplot(x['x1'], ax = ax1)
sns.kdeplot(x['x2'], ax = ax1)
ax2.set_title("After Robust Scaling")
sns.kdeplot(robust_scaled_df['x1'], ax = ax2)
sns.kdeplot(robust_scaled_df['x2'], ax = ax2)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA

X_norm.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size = 0.2)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)

X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
X_train=np.delete(X_train,-1,1)
X_test=np.delete(X_test,-1,1)

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import ExtraTreeClassifier
extra_tree = ExtraTreeClassifier(random_state = 0)

cls = BaggingClassifier(extra_tree, random_state = 0).fit(X_train, y_train)
cls.score(X_test, y_test)

y_pred = cls.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

class_names = pd.unique(df["Label"])

print(__doc__)

import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_classification
from sklearn.ensemble import ExtraTreesClassifier

# Build a classification task using 3 informative features
X, y = make_classification(n_samples = 1000, n_features = 11, n_informative = 3, n_redundant = 0, n_repeated = 0, n_classes = 2, random_state = 0, shuffle = False)

# Build a forest and compute the impurity-based feature importances
forest = ExtraTreesClassifier(n_estimators = 250, random_state = 0)
forest.fit(X, y)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis = 0)
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(X.shape[1]):
  print("%d, %s (%f)" %(f+1, df["Label"], importances[indices[f]]))

# Plot the impurity-based feature importances of the forest
plt.figure()
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices], color = "r", yerr = std[indices], align = "center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()

# explore extra trees number of trees effect on performance
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import ExtraTreesClassifier
from matplotlib import pyplot

# get the dataset
def get_dataset():
  X, y = make_classification(n_samples = 1000, n_features = 20, n_informative = 15, n_redundant = 5, random_state = 4)
  return X, y

# get a list of models to evaluate
def get_models():
  models = dict()
  # define no. of trees to consider
  n_trees = [10, 50, 100, 500, 1000, 5000]
  for n in n_trees:
    models[str(n)] = ExtraTreesClassifier(n_estimators = n)
  return models

# evaluate a given model using cross-validation
def evaluate_model(model, X, y):
  # define the evaluation procedure
  cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)
  # evaluate model and collect the results
  scores = cross_val_score(model, X, y, scoring = "accuracy", cv = cv, n_jobs = 1)
  return scores

# define dataset
X, y = get_dataset()
# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()

for name, model in models.items():
  # evaluate the model
  scores = evaluate_model(model, X, y)
  # store the results
  results.append(scores)
  names.append(name)
  # summarize the performance along the way
  print(">%s %.3f (%.3f)" % (name, mean(scores), std(scores)))

# plot model performance for comparison
pyplot.boxplot(results, labels = names, showmeans = True)
pyplot.show()

"""The end.."""
